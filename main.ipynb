{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.33840081 -0.05100334  0.04788153 ...  0.40067028  0.19608252\n",
      "   1.        ]\n",
      " [ 0.06734818 -0.03716935 -0.10719868 ...  0.18490265  0.18538352\n",
      "   1.        ]\n",
      " [ 0.05682186  0.07745515  0.17622378 ...  0.08946696  0.28167453\n",
      "   1.        ]\n",
      " ...\n",
      " [-0.30370445  0.03200061  0.23504731 ... -0.2134376  -0.23116427\n",
      "   1.        ]\n",
      " [-0.15107287 -0.06681362 -0.02163719 ... -0.07235876 -0.31960935\n",
      "   1.        ]\n",
      " [-0.23791498  0.46085436  0.02114356 ... -0.15949569 -0.14985186\n",
      "   1.        ]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt('wine.data', delimiter=',')\n",
    "\n",
    "X, y = data[:, 1:], data[:,0]\n",
    "\n",
    "# tranform problem into binary classification task\n",
    "idxs = [i for i in range(len(y)) if y[i] == 1 or y[i] == 2]\n",
    "        \n",
    "X, y = X[idxs], y[idxs]\n",
    "\n",
    "# normalize data\n",
    "X = (X - X.mean(axis=0))/(X.max(axis=0) - X.min(axis=0))\n",
    "X = np.hstack((X,np.ones(len(X)).reshape(len(X),1)))\n",
    "\n",
    "# transform target variable\n",
    "y = np.array(list(map(lambda x: 0 if x == 1 else 1, y))) \n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-23.68460805 -10.12066599 -21.12039252  22.84649101  -1.88806639\n",
      "    3.1722054   -7.75475242   0.25009727   1.29090767 -11.88895965\n",
      "    5.2113917  -10.49802832 -31.92757813  -0.11872858]]\n",
      "Loss L* = 0.000387        \n"
     ]
    }
   ],
   "source": [
    "# scikit learn implementation (our benchmark)\n",
    "reg = LogisticRegression(solver='sag', C=100000, max_iter = 10000).fit(X,y)\n",
    "print(reg.coef_)\n",
    "L_star = log_loss(y,reg.predict_proba(X))\n",
    "\n",
    "print(\"Loss L* = {:<16f}\".format(log_loss(y,reg.predict_proba(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def cross_entropy_loss(y,y_pred):\n",
    "    loss = -(1/len(y))*np.sum(y*np.log(y_pred) + (1 - y)*np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "def cross_entropy_grad(y,y_pred,x):\n",
    "    return list(np.dot((y_pred - y),X)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 1) (130, 14) (130,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 6720.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for best coordinate descent 0.9307692307692308\n",
      "Loss L = 0.5832867593722367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# initial weights\n",
    "w = np.zeros(14).reshape(14,1)\n",
    "w_rand = w\n",
    "\n",
    "print(w.shape, X.shape, y.shape)\n",
    "\n",
    "eta = 0.01\n",
    "loss = []\n",
    "loss_rand = []\n",
    "num_iter = 10 # just take a bunch of iterations\n",
    "for t in tqdm(range(num_iter)):\n",
    "    \n",
    "    # predict step \n",
    "    y_pred = sigmoid(np.dot(w.T,X.T))\n",
    "    grad = cross_entropy_grad(y,y_pred,X)\n",
    "    \n",
    "    largest = np.argmax(np.abs(grad)) # idx with largest magnitude\n",
    "    loss.append(cross_entropy_loss(y,y_pred))  \n",
    "    \n",
    "    # update that coordinate with largest gradient in magnitude\n",
    "    w[largest] = w[largest] - eta*grad[largest]\n",
    "\n",
    "y_pred = sigmoid(np.dot(w.T,X.T))\n",
    "y_pred = np.array(list(map(lambda x: 1 if x >= 0.5 else 0, y_pred.flatten()))) # post processing\n",
    "\n",
    "print(\"Accuracy for best coordinate descent {0}\".format(accuracy_score(y, y_pred)))\n",
    "print(\"Loss L = {0}\".format(loss[-1]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
